torchrun --nnodes 1 --nproc-per-node 4 ./src/finetuning.py \
--model_name "TinyLlama/TinyLlama-1.1B-Chat-v1.0" \
--tokenizer_name "TinyLlama/TinyLlama-1.1B-Chat-v1.0" \
--enable_fsdp \
--use_fp32 \
--low_cpu_fsdp \
--lr=3e-5 \
--weight_decay=0.1 \
--quant_type "nf4" \
--batching_strategy="padded" \
--fsdp_activation_checkpointing=false \
--batch_size_training=1 \
--mixed_precision=false \
--log_steps=5 \
--flop_counter=true\
--gradient_accumulation_steps=2 \
