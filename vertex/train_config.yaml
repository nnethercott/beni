run_name: "recap_quick_train"
output_dir: "aip-model-dir"
text_name_or_path: "meta-llama/Llama-3.2-1B"
vision_name_or_path: "google/siglip-so400m-patch14-384"
deepspeed: "/app/scripts/zero2.json" # location in container
n_concat_tokens: 9
vision_cls: "SiglipVisionModel"
freeze: true
unfreeze_lm_head: false
attn_implementation: "eager"
feature_select_index: -2
use_cls: true
img_size: 384
use_global_crop: false
instruction_template: "'<|begin_of_text|>{instruction}\n'" # need ' to escape for hf parser
response_template: "'{response}<|end_of_text|>'" 
lora_r: 4
lora_alpha: 32
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"
enable_peft: false
per_device_train_batch_size: 12
gradient_accumulation_steps: 1
learning_rate: 0.0004
lr_scheduler_type: "cosine_with_min_lr"
lr_scheduler_kwargs: '{"min_lr": 0.00004}'
weight_decay: 0.0
label_names: "labels"
adam_beta1: 0.9
adam_beta2: 0.999
max_grad_norm: 1.0
num_train_epochs: 3
warmup_ratio: 0.03
logging_strategy: "steps"
logging_steps: 1
save_strategy: "epoch"
save_total_limit: 3
save_only_model: true
save_steps: 1000
seed: 42
optim: "adamw_torch"
report_to: "wandb"
dataloader_num_workers: 2
dataloader_pin_memory: true
tf32: false
group_by_length: false
ddp_find_unused_parameters: false
half_precision_backend: "auto"
fp16: true
dataset_name: "recap"
n_samples: 10000
